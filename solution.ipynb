{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 输入特征\n",
    "\n",
    "5-day 自回归\n",
    "\n",
    "# 特征建模\n",
    "\n",
    "[开盘价，最高价-开盘价，最低价-开盘价，收盘价-开盘价，交易量]\n",
    "\n",
    "## 纳斯达克指数数据解释\n",
    "Date: 2015-01-02，表示交易日期。\n",
    "Ticker: IXIC，代表纳斯达克综合指数。\n",
    "Open: 4760.240234375，开盘价。\n",
    "High: 4777.009765625，当天最高价。\n",
    "Low: 4698.10986328125，当天最低价。\n",
    "Close: 4726.81005859375，收盘价。\n",
    "Adjusted: 4726.81005859375，调整后的收盘价（考虑分红、拆股等因素）。\n",
    "Returns: nan，收益率（此处为缺失值）。\n",
    "Volume: 1435150000，交易量。\n",
    "## 苹果公司股票数据解释\n",
    "Date: 2015-01-05，交易日期。\n",
    "Ticker: AAPL，代表苹果公司。\n",
    "Open: 27.07250022888184，开盘价。\n",
    "High: 27.162500381469727，当天最高价。\n",
    "Low: 26.352500915527344，当天最低价。\n",
    "Close: 26.5625，收盘价。\n",
    "Adjusted: 23.63528251647949，调整后的收盘价。\n",
    "Returns: -0.0281718672358495，收益率（表示相对于前一日的涨跌比率）。\n",
    "Volume: 257142000，交易量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c6a637fa731b13d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 创建模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "238ba4a93273edfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from deeplotx import AutoRegression\n",
    "model = AutoRegression(feature_dim=5, hidden_dim=128, recursive_layers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T10:25:47.357135Z",
     "start_time": "2025-05-11T10:25:39.508672Z"
    }
   },
   "id": "438e21a606119ca",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 选择数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "450a8a2085b87ee7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from data_preprocess import get_dataset\n",
    "batch_size = 16\n",
    "train_loader, valid_loader = get_dataset('AAPL', batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T10:28:54.136015Z",
     "start_time": "2025-05-11T10:28:53.916787Z"
    }
   },
   "id": "3faffe973982abc3",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e70fa833b4c56f5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 16\u001B[39m\n\u001B[32m     14\u001B[39m total_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m batch_texts, batch_labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m     outputs = \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_texts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m     loss = criterion(outputs, batch_labels) + model.elastic_net(alpha=elastic_net_param[\u001B[33m'\u001B[39m\u001B[33malpha\u001B[39m\u001B[33m'\u001B[39m], rho=elastic_net_param[\u001B[33m'\u001B[39m\u001B[33mrho\u001B[39m\u001B[33m'\u001B[39m])\n\u001B[32m     18\u001B[39m     optim.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\large_project\\stock_price_prediction\\.venv\\Lib\\site-packages\\deeplotx\\nn\\recursive_sequential.py:28\u001B[39m, in \u001B[36mRecursiveSequential.forward\u001B[39m\u001B[34m(self, x, state)\u001B[39m\n\u001B[32m     26\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, state: \u001B[38;5;28mtuple\u001B[39m[torch.Tensor, torch.Tensor]) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor, \u001B[38;5;28mtuple\u001B[39m[torch.Tensor, torch.Tensor]]:\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     x, (hidden_state, cell_state) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     29\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.regressive_head(x[:, -\u001B[32m1\u001B[39m, :])\n\u001B[32m     30\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x, (hidden_state, cell_state)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\large_project\\stock_price_prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\large_project\\stock_price_prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Desktop\\large_project\\stock_price_prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1116\u001B[39m, in \u001B[36mLSTM.forward\u001B[39m\u001B[34m(self, input, hx)\u001B[39m\n\u001B[32m   1111\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m hx[\u001B[32m0\u001B[39m].dim() != \u001B[32m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m hx[\u001B[32m1\u001B[39m].dim() != \u001B[32m2\u001B[39m:\n\u001B[32m   1112\u001B[39m         msg = (\n\u001B[32m   1113\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mFor unbatched 2-D input, hx and cx should \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1114\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33malso be 2-D but got (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhx[\u001B[32m0\u001B[39m].dim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-D, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhx[\u001B[32m1\u001B[39m].dim()\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m-D) tensors\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1115\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1116\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg)\n\u001B[32m   1117\u001B[39m     hx = (hx[\u001B[32m0\u001B[39m].unsqueeze(\u001B[32m1\u001B[39m), hx[\u001B[32m1\u001B[39m].unsqueeze(\u001B[32m1\u001B[39m))\n\u001B[32m   1118\u001B[39m \u001B[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001B[39;00m\n\u001B[32m   1119\u001B[39m \u001B[38;5;66;03m# the user believes he/she is passing in.\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "num_epochs = 10\n",
    "elastic_net_param = {'alpha': 1e-4, 'rho': 0.2}\n",
    "learning_rate = 2e-6\n",
    "train_loss_threshold = 0.\n",
    "valid_loss_threshold = 0.\n",
    "criterion = nn.MSELoss()\n",
    "optim = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_texts, batch_labels in train_loader:\n",
    "        outputs = model.forward(batch_texts, model.initial_state(batch_size=batch_size))\n",
    "        loss = criterion(outputs, batch_labels) + model.elastic_net(alpha=elastic_net_param['alpha'], rho=elastic_net_param['rho'])\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_loss += loss.item()\n",
    "    if epoch % 3 == 0:\n",
    "        total_valid_loss = 0.0\n",
    "        for batch_texts, batch_labels in valid_loader:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                outputs = model.forward(batch_texts, model.initial_state(batch_size=batch_size))\n",
    "                loss = criterion(outputs, batch_labels) + model.elastic_net(alpha=elastic_net_param['alpha'], rho=elastic_net_param['rho'])\n",
    "                total_valid_loss += loss.item()\n",
    "                model.train()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {total_loss:.4f} | \"\n",
    "              f\"Valid Loss: {total_valid_loss:.4f}\")\n",
    "        if total_valid_loss <= valid_loss_threshold:\n",
    "            break\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Train Loss: {total_loss:.4f}\")\n",
    "    if total_loss <= train_loss_threshold:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-11T10:29:20.711859Z",
     "start_time": "2025-05-11T10:29:20.467996Z"
    }
   },
   "id": "61ee70038d86517f",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
